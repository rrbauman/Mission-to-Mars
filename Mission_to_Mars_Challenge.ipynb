{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "70e7e8af",
   "metadata": {},
   "outputs": [],
   "source": [
    "### 10.3.3\n",
    "# Import Splinter and BeautifulSoup\n",
    "from splinter import Browser\n",
    "from bs4 import BeautifulSoup as soup\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7839312b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "====== WebDriver manager ======\n",
      "Current google-chrome version is 91.0.4472\n",
      "Get LATEST driver version for 91.0.4472\n",
      "Get LATEST driver version for 91.0.4472\n",
      "Trying to download new driver from https://chromedriver.storage.googleapis.com/91.0.4472.101/chromedriver_win32.zip\n",
      "Driver has been saved in cache [C:\\Users\\bdydh\\.wdm\\drivers\\chromedriver\\win32\\91.0.4472.101]\n"
     ]
    }
   ],
   "source": [
    "# set executable path and set up the URL:\n",
    "\n",
    "executable_path = {'executable_path': ChromeDriverManager().install()}\n",
    "browser = Browser('chrome', **executable_path, headless=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "52e3de41",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# In the next cell of your Jupyter notebook, we'll assign the url and instruct the browser to visit it.\n",
    "# One is that we're searching for elements with a specific combination of tag (div) and attribute (list_text). \n",
    "# Secondly, we're also telling our browser to wait one second before searching for components\n",
    "# Visit the mars nasa news site\n",
    "url = 'https://redplanetscience.com'\n",
    "browser.visit(url)\n",
    "# Optional delay for loading the page\n",
    "browser.is_element_present_by_css('div.list_text', wait_time=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "783041ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up the HTML parser:\n",
    "## The . is used for selecting classes, such as list_text, so the code 'div.list_text' pinpoints the <div /> tag \n",
    "# with the class of list_text\n",
    "# In this line of code, we chained .find onto our previously assigned variable, slide_elem. When we do this, \n",
    "# we're saying, \"This variable holds a ton of information, so look inside of that information to find this \n",
    "# specific data.\" The data we're looking for is the content title, which we've specified by saying, \n",
    "# \"The specific data is in a <div /> with a class of 'content_title'.\"\n",
    "\n",
    "\n",
    "html = browser.html\n",
    "news_soup = soup(html, 'html.parser')\n",
    "slide_elem = news_soup.select_one('div.list_text')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "76f21fd1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<div class=\"content_title\">Robotic Toolkit Added to NASA's Mars 2020 Rover</div>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We'll want to assign the title and summary text to variables we'll reference later. In the next empty cell, \n",
    "# let's begin our scraping. Type the following:\n",
    "\n",
    "slide_elem.find('div', class_='content_title')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b9df77e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Robotic Toolkit Added to NASA's Mars 2020 Rover\""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we need to get just the text, and the extra HTML stuff isn't necessary. In the next cell, type the following:\n",
    "\n",
    "# Use the parent element to find the first `a` tag and save it as `news_title`\n",
    "news_title = slide_elem.find('div', class_='content_title').get_text()\n",
    "news_title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "87da4898",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"The bit carousel, which lies at the heart of the rover's Sample Caching System, is now aboard NASA's newest rover. \""
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use the parent element to find the paragraph text\n",
    "\n",
    "news_p = slide_elem.find('div', class_='article_teaser_body').get_text()\n",
    "news_p"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f7ff363",
   "metadata": {},
   "source": [
    "### 10.3.4\n",
    "# In your Jupyter notebook, use markdown to separate the article scraping \n",
    "# from the image scraping.\n",
    "# change format of code to markdown on the top menu (where you reset kernel # etc)\n",
    "\n",
    "### Featured Images\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72dfceae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visit URL\n",
    "url = 'https://spaceimages-mars.com'\n",
    "browser.visit(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6606c892",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find and click the full image button\n",
    "# Notice the indexing chained at the end of the first line of code? \n",
    "# With this, we've stipulated that we want our browser to click the second button.\n",
    "\n",
    "full_image_elem = browser.find_by_tag('button')[1]\n",
    "full_image_elem.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17bbd2b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "## With the new page loaded onto our automated browser, it needs to be parsed \n",
    "## so we can continue and scrape the full-size image URL.\n",
    "\n",
    "# Parse the resulting html with soup\n",
    "html = browser.html\n",
    "img_soup = soup(html, 'html.parser')\n",
    "img_soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ad43727",
   "metadata": {},
   "outputs": [],
   "source": [
    "## We'll use the image tag and class (<img />and fancybox-img) to build the URL to the full-size image.\n",
    "# Find the relative image url\n",
    "img_url_rel = img_soup.find('img', class_='fancybox-image').get('src')\n",
    "img_url_rel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adc3bb93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's add the base URL to our code.\n",
    "# We're using an f-string for this print statement because it's a cleaner way to create print statements; \n",
    "# they're also evaluated at run-time. \n",
    "# Use the base URL to create an absolute URL\n",
    "img_url = f'https://spaceimages-mars.com/{img_url_rel}'\n",
    "img_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d13a18b",
   "metadata": {},
   "outputs": [],
   "source": [
    "### 10.3.5\n",
    "# Instead of scraping each row, or the data in each <td />, we're going to scrape the entire table \n",
    "# with Pandas' .read_html() function.\n",
    "# With this line, we're creating a new DataFrame from the HTML table.\n",
    "# we assign columns to the new DataFrame for additional clarity.\n",
    "# By using the .set_index() function, we're turning the Description column into the \n",
    "# DataFrame's index. inplace=True means that the updated index will remain in place, \n",
    "# without having to reassign the DataFrame to a new variable.\n",
    "\n",
    "df = pd.read_html('https://galaxyfacts-mars.com')[0]\n",
    "df.columns=['description', 'Mars', 'Earth']\n",
    "df.set_index('description', inplace=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83e1e1ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pandas also has a way to easily convert our DataFrame back into HTML-ready code using the .to_html() function\n",
    "df.to_html()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fb49d6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Use browser to visit the URL \n",
    "url = 'https://marshemispheres.com/'\n",
    "browser.visit(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dd1bfb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Create a list to hold the images and titles.\n",
    "hemisphere_image_urls = []\n",
    "\n",
    "# 3. Write code to retrieve the image urls and titles for each hemisphere.\n",
    "# Parse html:\n",
    "html_hemisphere = browser.html\n",
    "img_title_soup = soup(html_hemisphere, 'html.parser')\n",
    "###print(img_title_soup)\n",
    "image_page = img_title_soup.find_all('div',class_='description')\n",
    "#print(image_page)\n",
    "\n",
    "for image in image_page:\n",
    "    title = image.find('h3').text\n",
    "    mars_img = image.find('a',class_='itemLink product-item')\n",
    "    url_end = mars_img.get('href')\n",
    "    \n",
    "    img_url = f'https://marshemispheres.com/{url_end}'\n",
    "    \n",
    "    main_url = browser.visit(img_url)  \n",
    "    html_rose = browser.html\n",
    "    img_title_mars = soup(html_rose, 'html.parser')\n",
    "    img_url = img_title_mars.find('li').find('a').get('href')\n",
    "\n",
    "    img_url = f'https://marshemispheres.com/{img_url}'\n",
    "    \n",
    "    # add to dict:\n",
    "    data_dict = {\n",
    "        'img_url':img_url,\n",
    "        'title':title\n",
    "    }\n",
    "    # append to the list:\n",
    "    hemisphere_image_urls.append(data_dict)\n",
    "    # and return to the main url:\n",
    "print(hemisphere_image_urls)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8da70fb7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c39b576",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Print the list that holds the dictionary of each image url and title.\n",
    "hemisphere_image_urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cbe1572",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Quit the browser\n",
    "browser.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdbb169b",
   "metadata": {},
   "outputs": [],
   "source": [
    "###items = img_title_soup.find_all('div', class_='item')\n",
    "#print(items)\n",
    "###for item in items:\n",
    "    # Create a dictionary list to hold the output:\n",
    "    ###data_dict = {}\n",
    "    # Open page with the image:\n",
    "    ###image_page = img_title_soup.find('a',class_='itemLink product-item')\n",
    "    #rint(image_page)\n",
    "    ###browser.visit(f'https://marshemispheres.com/{image_page}')\n",
    "    # parse image\n",
    "    ###image_html = browser.html\n",
    "    ###img_page_soup = soup(image_html, 'html.parser')\n",
    "    # get url from sample:abs\n",
    "    # image = img_page_soup.find('ul').li.a['href']\n",
    "    ###image = img_page_soup.find('img', class_='thumb').get('src')\n",
    "    \n",
    "    # append url:\n",
    "    ###img_url = f'https://marshemispheres.com/{image}'\n",
    "    # title:\n",
    "    ###title = img_page_soup.find('h3').text\n",
    "    # add to dict:\n",
    "    ###data_dict = {\n",
    "    ###    'img_url':img_url,\n",
    "    ###    'title':title\n",
    "    ###}\n",
    "    # append to the list:\n",
    "    ###hemisphere_image_urls.append(data_dict)\n",
    "    # and return to the main url:\n",
    "\n",
    "#  main_url = browser.visit(url)\n",
    "\n",
    "#main_url = browser.visit('https://marshemispheres.com/cerberus.html') \n",
    "#html_rose = browser.html\n",
    "#img_title_mars = soup(html_rose, 'html.parser')\n",
    "#img_title_mars.find('li')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PythonData",
   "language": "python",
   "name": "pythondata"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
